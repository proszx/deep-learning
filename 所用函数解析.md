#python构造类

## super的使用
如果子类(Puple)继承父类(Person)不做初始化，那么会自动继承父类(Person)属性name。
如果子类(Puple_Init)继承父类(Person)做了初始化，且不调用super初始化父类构造函数，那么子类(Puple_Init)不会自动继承父类的属性(name)。
如果子类(Puple_super)继承父类(Person)做了初始化，且调用了super初始化了父类的构造函数，那么子类(Puple_Super)也会继承父类的(name)属性。

# numpy 函数解析
### np.eye
#### 原文注释解析
eye(N, M=None, k=0, dtype=float, order='C')
Return a 2-D array with ones on the diagonal and zeros elsewhere.

Parameters
N : int Number of rows in the output.

M : int, optional Number of columns in the output. If None, defaults to N. 

k : int, optional Index of the diagonal: 0 (the default) refers to the main diagonal, a positive value refers to an upper diagonal, and a negative value to a lower diagonal. 

dtype : data-type, optional Data-type of the returned array. 

order : {'C', 'F'}, optional Whether the output should be stored in row-major (C-style) or column-major (Fortran-style) order in memory.

Returns

I : ndarray of shape (N,M) An array where all elements are equal to zero, except for the k-th diagonal, whose values are equal to one.

See Also
identity : (almost) equivalent function diag : diagonal 2-D array from a 1-D array specified by the user.

Examples
>>> np.eye(2, dtype=int)
array([[1, 0],
       [0, 1]])
>>> np.eye(3, k=1)
array([[ 0.,  1.,  0.],
       [ 0.,  0.,  1.],
       [ 0.,  0.,  0.]])

重点关注第一个第三个参数

第一个参数：输出方阵（行数=列数）的规模，即行数或列数

第三个参数：默认情况下输出的是对角线全“1”，其余全“0”的方阵，如果k为正整数，则在右上方第k条对角线全“1”其余全“0”，k为负整数则在左下方第k条对角线全“1”其余全“0”。

在制造数据时候有用到
具体如下所示
```
>>> import numpy as np
>>> np.eye(7)
array([[1., 0., 0., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0.],
       [0., 0., 1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0., 0., 0.],
       [0., 0., 0., 0., 1., 0., 0.],
       [0., 0., 0., 0., 0., 1., 0.],
       [0., 0., 0., 0., 0., 0., 1.]])
>>> np.eye(7)[0,5]
0.0
>>> np.eye(7)[[0,5]]
array([[1., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 1., 0.]])

```
用一个7*7的方针表示整个数据集的字典，作为简单embedding编码，之后对于第一句话
"i like dog"，这里的提取前两个词作为输入，后面一个词作为输出，相应的用矩阵表示“i like” 则为
```
array([[1., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 1., 0.]])
```
# torch api解析
## torch.nn
### nn.Parameter()
首先可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter绑定到这个module里面(net.parameter()中就有这个绑定的parameter，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。

## torch.optim

 `torch.optim`是一个实现了各种优化算法的库。大部分常用的方法得到支持，并且接口具备足够的通用性，使得未来能够集成更加复杂的方法。 

### 如何使用optimizer

 为了使用`torch.optim`，你需要构建一个optimizer对象。这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。 

### 构建

 为了构建一个`Optimizer`，你需要给它一个包含了需要优化的参数（必须都是`Variable`对象）的iterable。然后，你可以设置optimizer的参 数选项，比如学习率，权重衰减，等等。 

例子：

`optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) optimizer = optim.Adam([var1, var2], lr = 0.0001) `

### 为每个参数单独设置选项

`Optimizer`也支持为每个参数单独设置选项。若想这么做，不要直接传入`Variable`的iterable，而是传入`dict`的iterable。每一个dict都分别定义了一组参数，并且包含一个`param`键，这个键对应参数的列表。其他的键应该optimizer所接受的其他参数的关键字相匹配，并且会被用于对这组参数的优化。

注意：

你仍然能够传递选项作为关键字参数。在未重写这些选项的组中，它们会被用作默认值。当你只想改动一个参数组的选项，但其他参数组的选项不变时，这是 非常有用的。

例如，当我们想指定每一层的学习率时，这是非常有用的：

` optim.SGD([               ` 

 								` {'params': model.base.parameters()},                ` 

 								`{'params': model.classifier.parameters(), 'lr': 1e-3}`

​							 `            ], lr=1e-2, momentum=0.9) `

 这意味着`model.base`的参数将会使用`1e-2`的学习率，`model.classifier`的参数将会使用`1e-3`的学习率，并且`0.9`的momentum将会被用于所 有的参数。 

### 进行单次优化

 所有的optimizer都实现了`step()`方法，这个方法会更新所有的参数。它能按两种方式来使用： 

 **`optimizer.step()`** 

 这是大多数optimizer所支持的简化版本。一旦梯度被如`backward()`之类的函数计算好后，我们就可以调用这个函数。 

 例子 

```
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
```

**`optimizer.step(closure)`**

一些优化算法例如Conjugate Gradient和LBFGS需要重复多次计算函数，因此你需要传入一个闭包去允许它们重新计算你的模型。这个闭包应当清空梯度， 计算损失，然后返回。

例子：

```
for input, target in dataset:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss
    optimizer.step(closure)
```

