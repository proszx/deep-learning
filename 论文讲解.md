# A Neural Probabilistic Language Model
这篇文章是 Bengio 等人在 2001 年发表在 NIPS 上的文章，先看题目，一个基于神经概率算法的语言模型。

## abstract
先看摘要：统计语言建模的目标是学习语言中单词序列的联合概率函数。由于维度的诅咒，这本质上是困难的：测试模型的单词序列可能与训练期间看到的所有单词序列不同。基于n-gram的传统但非常成功的方法通过连接训练集中看到的非常短的重叠序列来获得泛化。我们建议通过学习单词的分布式表示来对抗维度的诅咒，这允许每个训练句子向模型通知指数个语义上相邻的句子。该模型同时学习（1）每个单词的分布式表示以及（2）单词序列的概率函数，用这些表示表示。获得泛化是因为如果由与形成已经看到的句子的单词相似（在具有附近表示的意义上）的单词构成，则之前从未见过的单词序列具有高概率。在合理的时间内训练这样的大型模型（具有数百万个参数）本身就是一个重大挑战。我们报告了使用神经网络进行概率函数的实验，在两个文本语料库中显示所提出的方法显着改进了最先进的n-gram模型，并且所提出的方法允许利用更长的上下文。

上述这段我用的是谷歌翻译 也存在部分语法不通顺，但是很容易就能去找到重点。

* 1)统计语言建模的目标是学习语言中单词序列的联合概率函数
* 2)通过学习单词的分布式表示来对抗维度的诅咒，这允许每个训练句子向模型通知指数个语义上相邻的句子
* 3) 模型的创新 1）每个单词的分布式表示以及（2）单词序列的概率函数
* 4）使用神经网络进行概率函数的实验，在两个文本语料库中显示所提出的方法显着改进了最先进(SOTA state of the art )的n-gram模型，并且所提出的方法允许利用更长的上下文
  
所提炼出来的四个要点 本质上说明了bengio 阐述了传统语言建模的定义。此外也说明了他解决维度问题的思路。 阐述模型的要义，模型的创新所达到的效果这四个部分。

## introduction
introduction 我一般是习惯性进行忽略的 因为大都是阐述前人和相关工作。
但是这里还是选择仔细阐述下。
作者首先阐述了传统的统计语言模型，也就是常见的贝叶斯概率公式变种。给定前面词后面一个词出现的条件概率：
![f1](http://images.cnitblog.com/blog/590456/201409/012034441256455.png)
## 