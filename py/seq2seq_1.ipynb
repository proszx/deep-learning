{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import TranslationDataset,Multi30k\n",
    "from torchtext.data import Field,BucketIterator\n",
    "device=torch.device('cuda:0')\n",
    "import spacy\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1024\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n",
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "spacy_de=spacy.load('de')\n",
    "spacy_en=spacy.load('en')\n",
    "def token_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "def token_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "SRC=Field(tokenize=token_de,init_token='<sos>',eos_token='<eos>',lower=True)\n",
    "TRG=Field(tokenize=token_en,init_token='<sos>',eos_token='<eos>',lower=True)\n",
    "train_data,valid_data,test_data=Multi30k.splits(exts=('.de','.en'),fields=(SRC,TRG))\n",
    "\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7855\n",
      "5893\n",
      "<torchtext.data.iterator.BucketIterator object at 0x7f988475ef28>\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data,min_freq=2)\n",
    "TRG.build_vocab(train_data,min_freq=2)\n",
    "print(len(SRC.vocab))\n",
    "print(len(TRG.vocab))\n",
    "BATCH_SIZE=128\n",
    "train_iter,valid_iter,test_iter=BucketIterator.splits((train_data,valid_data,test_data),batch_size=BATCH_SIZE,device=device)\n",
    "print(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,emb_dim,hid_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim=input_dim\n",
    "        self.emb_dim=emb_dim\n",
    "        self.hid_dim=hid_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.dropout=dropout\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,src):\n",
    "        embedded=self.dropout(self.embedding(src))\n",
    "        \n",
    "        outputs,(hidden,cell)=self.rnn(embedded)\n",
    "        \n",
    "        return hidden,cell\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,emb_dim,hid_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim=emb_dim\n",
    "        self.hid_dim=hid_dim\n",
    "        self.output_dim=output_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        self.embedding=nn.Embedding(output_dim,emb_dim)\n",
    "        \n",
    "        self.rnn=nn.LSTM(emb_dim,hid_dim,n_layers,dropout=dropout)\n",
    "        \n",
    "        self.out=nn.Linear(hid_dim,output_dim)\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,inputs,hidden,cell):\n",
    "        inputs=inputs.unsqueeze(0)\n",
    "        embedded=self.dropout(self.embedding(inputs))\n",
    "        \n",
    "        output,(hidden,cell)=self.rnn(embedded,(hidden,cell))\n",
    "        \n",
    "        prediction=self.out(output.squeeze(0))\n",
    "        \n",
    "        return prediction,hidden,cell\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.device=device\n",
    "        \n",
    "        assert encoder.hid_dim==decoder.hid_dim,\\\n",
    "            \"编码层和解码层隐藏层必须相同\"\n",
    "        assert encoder.n_layers==decoder.n_layers,\\\n",
    "            \"编码器解码器层数必须相同\"\n",
    "    def forward(self,src,trg,teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        batch_size=trg.shape[1]\n",
    "        max_len=trg.shape[0]\n",
    "        \n",
    "        trg_vacab_size=self.decoder.output_dim\n",
    "        \n",
    "        outputs=torch.zeros(max_len,batch_size,trg_vacab_size).to(self.device)\n",
    "        \n",
    "        hidden,cell=self.encoder(src)\n",
    "        \n",
    "        inputs=trg[0,:]\n",
    "        \n",
    "        for t in range(1,max_len):\n",
    "            output,hidden,cell=self.decoder(inputs,hidden,cell)\n",
    "            outputs[t]=output\n",
    "            teacher_forcing=random.random()<teacher_forcing_ratio\n",
    "            top1=output.max(1)[1]\n",
    "            inputs=(trg[t] if teacher_forcing else top1)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim=len(SRC.vocab)\n",
    "output_dim=len(TRG.vocab)\n",
    "\n",
    "encode_emb_dim=256\n",
    "decode_emb_dim=256\n",
    "\n",
    "hid_dim=512\n",
    "n_layers=2\n",
    "\n",
    "enc_dropout=0.5\n",
    "dec_dropout=0.5\n",
    "\n",
    "enc=Encoder(input_dim,encode_emb_dim,hid_dim,n_layers,enc_dropout)\n",
    "dec=Decoder(output_dim,decode_emb_dim,hid_dim,n_layers,dec_dropout)\n",
    "\n",
    "model=Seq2Seq(enc,dec,device).to(device)\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,899,013 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "optims=optim.Adam(model.parameters())\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src,trg)\n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 29s\n",
      "\tTrain Loss: 5.046 | Train PPL: 155.446\n",
      "\t Val. Loss: 5.015 |  Val. PPL: 150.716\n",
      "Epoch: 02 | Time: 0m 29s\n",
      "\tTrain Loss: 4.512 | Train PPL:  91.067\n",
      "\t Val. Loss: 4.809 |  Val. PPL: 122.615\n",
      "Epoch: 03 | Time: 0m 29s\n",
      "\tTrain Loss: 4.186 | Train PPL:  65.758\n",
      "\t Val. Loss: 4.642 |  Val. PPL: 103.710\n",
      "Epoch: 04 | Time: 0m 29s\n",
      "\tTrain Loss: 3.971 | Train PPL:  53.046\n",
      "\t Val. Loss: 4.399 |  Val. PPL:  81.355\n",
      "Epoch: 05 | Time: 0m 29s\n",
      "\tTrain Loss: 3.789 | Train PPL:  44.217\n",
      "\t Val. Loss: 4.227 |  Val. PPL:  68.488\n",
      "Epoch: 06 | Time: 0m 29s\n",
      "\tTrain Loss: 3.606 | Train PPL:  36.817\n",
      "\t Val. Loss: 4.169 |  Val. PPL:  64.629\n",
      "Epoch: 07 | Time: 0m 29s\n",
      "\tTrain Loss: 3.474 | Train PPL:  32.281\n",
      "\t Val. Loss: 4.111 |  Val. PPL:  61.002\n",
      "Epoch: 08 | Time: 0m 29s\n",
      "\tTrain Loss: 3.327 | Train PPL:  27.858\n",
      "\t Val. Loss: 3.961 |  Val. PPL:  52.484\n",
      "Epoch: 09 | Time: 0m 29s\n",
      "\tTrain Loss: 3.169 | Train PPL:  23.794\n",
      "\t Val. Loss: 3.897 |  Val. PPL:  49.244\n",
      "Epoch: 10 | Time: 0m 29s\n",
      "\tTrain Loss: 3.068 | Train PPL:  21.493\n",
      "\t Val. Loss: 3.870 |  Val. PPL:  47.950\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optims, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
